import os
from typing import Dict, List, Tuple
from experiments import Experiment
from deployment import Deployment
from collections import defaultdict
from copy import deepcopy
import json

class PSLExperiment(Experiment):
    """
    Everything is the same as Experiment, except that the servers are run as:
    ./server storage config.json
    ./server worker config.json
    ./server sequencer config.json
    """

    def generate_arbiter_script(self):
        """
        This is a copy of the generate_arbiter_script method in experiments.py.
        The only difference is that the servers are run as:
        ./server storage config.json
        """

        script_base = f"""#!/bin/bash
set -e
set -o xtrace

# This script is generated by the experiment pipeline. DO NOT EDIT.
SSH_CMD="ssh -o StrictHostKeyChecking=no -i {self.dev_ssh_key}"
SCP_CMD="scp -o StrictHostKeyChecking=no -i {self.dev_ssh_key}"

# SSH into each VM and run the binaries
"""
        # Plan the binaries to run
        for repeat_num in range(self.repeats):
            print("Running repeat", repeat_num)
            _script = script_base[:]
            for vm, bin_list in self.binary_mapping.items():
                for bin in bin_list:
                    if "node" in bin:
                        binary_name = "server worker"
                    elif "storage" in bin:
                        binary_name = "server storage"
                    elif "sequencer" in bin:
                        binary_name = "server sequencer"
                    elif "client" in bin:
                        binary_name = "client"
                    elif "controller" in bin:
                        binary_name = "controller"

                    _script += f"""
$SSH_CMD {self.dev_ssh_user}@{vm.public_ip} '{self.remote_workdir}/build/{binary_name} {self.remote_workdir}/configs/{bin}_config.json > {self.remote_workdir}/logs/{repeat_num}/{bin}.log 2> {self.remote_workdir}/logs/{repeat_num}/{bin}.err' &
PID="$PID $!"
"""
                    
            _script += f"""
# Sleep for the duration of the experiment
sleep {self.duration}

# Kill the binaries. First with a SIGINT, then with a SIGTERM, then with a SIGKILL
echo -n $PID | xargs -d' ' -I{{}} kill -2 {{}} || true
echo -n $PID | xargs -d' ' -I{{}} kill -15 {{}} || true
echo -n $PID | xargs -d' ' -I{{}} kill -9 {{}} || true
sleep 10

# Kill the binaries in SSHed VMs as well. Calling SIGKILL on the local SSH process might have left them orphaned.
# Make sure not to kill the tmux server.
# Then copy the logs back and delete any db files. Cleanup for the next run.
"""
            for vm, bin_list in self.binary_mapping.items():
                for bin in bin_list:
                    if "node" in bin:
                        binary_name = "server"
                    elif "storage" in bin:
                        binary_name = "server"
                    elif "sequencer" in bin:
                        binary_name = "server"
                    elif "client" in bin:
                        binary_name = "client"
                    elif "controller" in bin:
                        binary_name = "controller"
                
                # Copy the logs back
                    _script += f"""
$SSH_CMD {self.dev_ssh_user}@{vm.public_ip} 'pkill -2 -c {binary_name}' || true
$SSH_CMD {self.dev_ssh_user}@{vm.public_ip} 'pkill -15 -c {binary_name}' || true
$SSH_CMD {self.dev_ssh_user}@{vm.public_ip} 'pkill -9 -c {binary_name}' || true
$SSH_CMD {self.dev_ssh_user}@{vm.public_ip} 'rm -rf /data/*' || true
$SCP_CMD {self.dev_ssh_user}@{vm.public_ip}:{self.remote_workdir}/logs/{repeat_num}/{bin}.log {self.remote_workdir}/logs/{repeat_num}/{bin}.log || true
$SCP_CMD {self.dev_ssh_user}@{vm.public_ip}:{self.remote_workdir}/logs/{repeat_num}/{bin}.err {self.remote_workdir}/logs/{repeat_num}/{bin}.err || true
"""
                    
            _script += f"""
sleep 10
"""
                    
            # pkill -9 -c server also kills tmux-server. So we can't run a server on the dev VM.
            # It kills the tmux session and the experiment. And we end up with a lot of orphaned processes.

            with open(os.path.join(self.local_workdir, f"arbiter_{repeat_num}.sh"), "w") as f:
                f.write(_script + "\n\n")


    def get_vms(self, deployment: Deployment) -> Tuple[List, List, List]:
        if self.node_distribution == "uniform":
            node_vms = deployment.get_all_node_vms()
        elif self.node_distribution == "sev_only":
            node_vms = deployment.get_nodes_with_tee("sev")
        elif self.node_distribution == "tdx_only":
            node_vms = deployment.get_nodes_with_tee("tdx")
        elif self.node_distribution == "nontee_only":
            node_vms = deployment.get_nodes_with_tee("nontee")
        elif self.node_distribution.startswith("tag:"):
            node_vms = deployment.get_nodes_with_tag(self.node_distribution.split(":")[1])
        else:
            node_vms = deployment.get_wan_setup(self.node_distribution)
        
        if self.storage_distribution == "uniform":
            storage_vms = deployment.get_all_storage_vms()
        elif self.storage_distribution == "sev_only":
            storage_vms = deployment.get_nodes_with_tee("sev")
        elif self.storage_distribution == "tdx_only":
            storage_vms = deployment.get_nodes_with_tee("tdx")
        elif self.storage_distribution == "nontee_only":
            storage_vms = deployment.get_nodes_with_tee("nontee")
        elif self.storage_distribution.startswith("tag:"):
            storage_vms = deployment.get_nodes_with_tag(self.storage_distribution.split(":")[1])
        else:
            storage_vms = deployment.get_wan_setup(self.storage_distribution)
        
        if self.sequencer_distribution == "uniform":
            sequencer_vms = deployment.get_all_node_vms()
        elif self.sequencer_distribution == "sev_only":
            sequencer_vms = deployment.get_nodes_with_tee("sev")
        elif self.sequencer_distribution == "tdx_only":
            sequencer_vms = deployment.get_nodes_with_tee("tdx")
        elif self.sequencer_distribution == "nontee_only":
            sequencer_vms = deployment.get_nodes_with_tee("nontee")
        elif self.sequencer_distribution.startswith("tag:"):
            sequencer_vms = deployment.get_nodes_with_tag(self.sequencer_distribution.split(":")[1])
        else:
            sequencer_vms = deployment.get_wan_setup(self.sequencer_distribution)
        
        return node_vms, storage_vms, sequencer_vms

    def generate_watchlists(self, sequencer_names: List[str], worker_names: List[str]) -> Dict[str, List[str]]:
        """
        Generate a watchlist for each sequencer.
        If there is only one sequencer, it gets the first 4 workers.
        Otherwise, worker_names split evenly between the sequencers.
        """
        ret = defaultdict(list)
        if len(sequencer_names) == 1:
            ret[sequencer_names[0]] = worker_names[:4]
            return dict(ret)

        curr_sequencer_idx = 0
        for worker_name in worker_names:
            ret[sequencer_names[curr_sequencer_idx]].append(worker_name)
            curr_sequencer_idx = (curr_sequencer_idx + 1) % len(sequencer_names)
        return dict(ret)




    def generate_configs(self, deployment: Deployment, config_dir, log_dir):
        # If config_dir is not empty, assume the configs have already been generated
        if len(os.listdir(config_dir)) > 0:
            print("Skipping config generation for experiment", self.name)
            return
        # Number of nodes in deployment may be < number of nodes in deployment
        # So we reuse nodes.
        # As a default, each deployed node gets its unique port number
        # So there will be no port clash.

        rr_cnt = 0
        nodelist = []
        nodes = {}
        node_configs = {}
        node_list_for_crypto = {}
        self.binary_mapping = defaultdict(list)

        # TODO: Do this separately for node, storage, and sequencer.

        (node_vms, storage_vms, sequencer_vms) = self.get_vms(deployment)
        print("Node vms", node_vms)
        print("Storage vms", storage_vms)
        print("Sequencer vms", sequencer_vms)
        print("Client vms", deployment.get_all_client_vms())
        worker_names = []
        storage_names = []
        sequencer_names = []
        port = deployment.node_port_base

        for node_num in range(1, self.num_nodes+1):
            port += 1
            listen_addr = f"0.0.0.0:{port}"
            name = f"node{node_num}"
            domain = f"{name}.pft.org"

            _vm = node_vms[rr_cnt % len(node_vms)]
            self.binary_mapping[_vm].append(name)
            
            private_ip = _vm.private_ip
            rr_cnt += 1
            connect_addr = f"{private_ip}:{port}"

            nodelist.append(name[:])
            nodes[name] = {
                "addr": connect_addr,
                "domain": domain
            }

            worker_names.append(name)

            node_list_for_crypto[name] = (connect_addr, domain)

            config = deepcopy(self.base_node_config)
            config["net_config"]["name"] = name
            config["net_config"]["addr"] = listen_addr
            data_dir = os.path.join(self.data_dir, f"{name}-db")
            config["consensus_config"]["log_storage_config"]["RocksDB"]["db_path"] = str(data_dir)
            config["worker_config"]["state_storage_config"]["RocksDB"]["db_path"] = str(data_dir)

            node_configs[name] = config

        for node_num in range(1, self.num_storage_nodes+1):
            port += 1
            listen_addr = f"0.0.0.0:{port}"
            name = f"storage{node_num}"
            domain = f"{name}.pft.org"

            _vm = storage_vms[rr_cnt % len(storage_vms)]
            self.binary_mapping[_vm].append(name)

            storage_names.append(name)
            
            private_ip = _vm.private_ip
            rr_cnt += 1
            connect_addr = f"{private_ip}:{port}"
            
            nodelist.append(name[:])
            nodes[name] = {
                "addr": connect_addr,
                "domain": domain
            }

            node_list_for_crypto[name] = (connect_addr, domain)

            config = deepcopy(self.base_node_config)
            config["net_config"]["name"] = name
            config["net_config"]["addr"] = listen_addr
            data_dir = os.path.join(self.data_dir, f"{name}-db")
            config["consensus_config"]["log_storage_config"]["RocksDB"]["db_path"] = str(data_dir)

            node_configs[name] = config

        for node_num in range(1, self.num_sequencer_nodes+1):
            port += 1
            listen_addr = f"0.0.0.0:{port}"
            name = f"sequencer{node_num}"
            domain = f"{name}.pft.org"
            
            _vm = sequencer_vms[rr_cnt % len(sequencer_vms)]
            self.binary_mapping[_vm].append(name)
            
            private_ip = _vm.private_ip
            rr_cnt += 1
            connect_addr = f"{private_ip}:{port}"
            
            nodelist.append(name[:])
            nodes[name] = {
                "addr": connect_addr,
                "domain": domain
            }
            sequencer_names.append(name)

            node_list_for_crypto[name] = (connect_addr, domain)

            config = deepcopy(self.base_node_config)
            config["net_config"]["name"] = name
            config["net_config"]["addr"] = listen_addr
            data_dir = os.path.join(self.data_dir, f"{name}-db")
            config["consensus_config"]["log_storage_config"]["RocksDB"]["db_path"] = str(data_dir)

            node_configs[name] = config

            

        if self.client_region == -1:
            client_vms = deployment.get_all_client_vms()
        else:
            client_vms = deployment.get_all_client_vms_in_region(self.client_region)

        crypto_info = self.gen_crypto(config_dir, node_list_for_crypto, len(client_vms))
        

        print("Worker names", worker_names)
        print("Storage names", storage_names)
        print("Sequencer names", sequencer_names)

        gossip_downstream_worker_list = self.generate_multicast_tree(worker_names, 2)
        sequencer_watchlist_map = self.generate_watchlists(sequencer_names, worker_names)

        print("Gossip downstream worker list", gossip_downstream_worker_list)

        for k, v in node_configs.items():
            tls_cert_path, tls_key_path, tls_root_ca_cert_path,\
            allowed_keylist_path, signing_priv_key_path = crypto_info[k]

            v["net_config"]["nodes"] = deepcopy(nodes)

            if k in sequencer_names:
                v["consensus_config"]["node_list"] = storage_names[:]
            else:
                v["consensus_config"]["node_list"] = nodelist[:]

            if k in worker_names:
                v["worker_config"]["gossip_downstream_worker_list"] = gossip_downstream_worker_list[k]
            else:
                v["worker_config"]["gossip_downstream_worker_list"] = []

            # if True or k == storage_names[0]:
            v["consensus_config"]["learner_list"] = sequencer_names[:]

            if k in sequencer_names:
                v["consensus_config"]["watchlist"] = sequencer_watchlist_map.get(k, [])

            v["net_config"]["tls_cert_path"] = tls_cert_path
            v["net_config"]["tls_key_path"] = tls_key_path
            v["net_config"]["tls_root_ca_cert_path"] = tls_root_ca_cert_path
            v["rpc_config"]["allowed_keylist_path"] = allowed_keylist_path
            v["rpc_config"]["signing_priv_key_path"] = signing_priv_key_path
            v["worker_config"]["all_worker_list"] = worker_names[:]
            v["worker_config"]["storage_list"] = storage_names[:] 
            v["worker_config"]["sequencer_list"] = sequencer_names[:]

            # Only simulate Byzantine behavior in node1.
            if "evil_config" in v and v["evil_config"]["simulate_byzantine_behavior"] and k != "node1":
                v["evil_config"]["simulate_byzantine_behavior"] = False
                v["evil_config"]["byzantine_start_block"] = 0

            with open(os.path.join(config_dir, f"{k}_config.json"), "w") as f:
                json.dump(v, f, indent=4)

        num_clients_per_vm = [self.num_clients // len(client_vms) for _ in range(len(client_vms))]
        num_clients_per_vm[-1] += (self.num_clients - sum(num_clients_per_vm))

        client_start_index = 0
        for client_num in range(len(client_vms)):
            config = deepcopy(self.base_client_config)
            client = "client" + str(client_num + 1)
            config["net_config"]["name"] = client

            client_nodes = deepcopy(nodes)
            client_nodes = {k: v for k, v in client_nodes.items() if k in worker_names}
            config["net_config"]["nodes"] = client_nodes

            tls_cert_path, tls_key_path, tls_root_ca_cert_path,\
            allowed_keylist_path, signing_priv_key_path = crypto_info[client]

            config["net_config"]["tls_root_ca_cert_path"] = tls_root_ca_cert_path
            config["rpc_config"] = {"signing_priv_key_path": signing_priv_key_path}

            config["workload_config"]["num_clients"] = num_clients_per_vm[client_num]
            config["workload_config"]["duration"] = self.duration
            config["workload_config"]["start_index"] = client_start_index
            client_start_index += num_clients_per_vm[client_num]

            self.binary_mapping[client_vms[client_num]].append(client)

            with open(os.path.join(config_dir, f"{client}_config.json"), "w") as f:
                json.dump(config, f, indent=4)

        # Controller config
        config = deepcopy(self.base_client_config)
        name = "controller"
        config["net_config"]["name"] = name
        config["net_config"]["nodes"] = deepcopy(nodes)

        tls_cert_path, tls_key_path, tls_root_ca_cert_path,\
        allowed_keylist_path, signing_priv_key_path = crypto_info[name]

        config["net_config"]["tls_root_ca_cert_path"] = tls_root_ca_cert_path
        config["rpc_config"] = {"signing_priv_key_path": signing_priv_key_path}

        config["workload_config"]["num_clients"] = 1
        config["workload_config"]["duration"] = self.duration

        with open(os.path.join(config_dir, f"{name}_config.json"), "w") as f:
            json.dump(config, f, indent=4)

        if self.controller_must_run:
            self.binary_mapping[client_vms[0]].append(name)





